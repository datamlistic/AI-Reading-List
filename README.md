# AI Reading List
This repo contains the AI reading list of about 30 items which Ilya Sutskever supposedly gave to John Carmack back in 2020. Unfortunately, neither Ilya nor John oficially confirmed this list, but I still think it's a great read. 

I've also tried my best to organise the papers by topic and put them in a chronological order that you should read. Also, a brief explanation of each item in the list is available [here](https://www.youtube.com/playlist?list=PL8hTotro6aVGtPgLJ_TMKe8C8MDhHBZ4W) on my YouTubeðŸŽ¥ channel.

Without further ado, this is the list:
1. Attention Mechanism
    1. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473)
    2. [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)
    3. [Attention Is All You Need](https://arxiv.org/pdf/1706.03762)
    4. [Neural Turing Machines](https://arxiv.org/pdf/1410.5401)
    5. [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361)
2. Recurrent Neural Networks
    1. [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
    2. [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    3. [Recurrent Neural Network Regularization](https://arxiv.org/pdf/1409.2329)
    4. [Pointer Networks](https://arxiv.org/pdf/1506.03134)
    5. [Order Matters: Sequence to Sequence for Sets](https://arxiv.org/pdf/1511.06391)
3. Computer Vision
    1. [CS231n: Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io)
    2. [ImageNet Classification with Deep Convolutional Neural Networks](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
    3. [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385)
    4. [Identity Mappings in Deep Residual Networks](https://arxiv.org/pdf/1603.05027)
    5. [Multi-Scale Neural Context Aggregation by Dilated Convolutions](https://arxiv.org/pdf/1511.07122)
    6. [Variational Lossy Autoencoder](https://arxiv.org/pdf/1611.02731)
6. Kolmogorov Complexity
    1. [The First Law of Complexodynamics](https://scottaaronson.blog/?p=762)
    2. [Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automato](https://arxiv.org/pdf/1405.6903)
    3. [Kolmogorov Complexity and Algorithmic Randomness - page 434 onwards](https://www.lirmm.fr/~ashen/kolmbook-eng-scan.pdf)
    4. [Keeping the Neural Networks Simple by Minimizing the Description Length of the Weights](https://www.cs.toronto.edu/~hinton/absps/colt93.pdf)
    5. [A Tutorial Introduction to the Minimum Description Length Principle](https://arxiv.org/pdf/math/0406077)
7. Relational Reasoning
    1. [A Simple Neural Network Module for Relational Reasoning](https://arxiv.org/pdf/1706.01427)
    2. [Relational Recurrent Neural Networks](https://arxiv.org/pdf/1806.01822)
7. Other
    1. [Deep Speech 2: End-to-End Speech Recognition in English and Mandarin](https://arxiv.org/pdf/1512.02595)
    2. [GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism](https://arxiv.org/pdf/1811.06965)
    3. [Neural Message Passing for Quantum Chemistry](https://arxiv.org/pdf/1704.01212)
    4. [Machine Super Intelligence](https://www.vetta.org/documents/Machine_Super_Intelligence.pdf)



**Notes:** 
- Understanding these papers might be quite difficult if you're not familiar with machine learning concepts. Therefore, if you've never taken any courses in machine learning before, I wholeheartedly recommend starting with Andrew Ng's specializations on [Machine Learning](https://www.coursera.org/specializations/machine-learning-introduction) and [Deep Learning](https://www.coursera.org/specializations/deep-learning?) before diving into this list.
- Please feel free to create PRs with the modifications you consider necessary to this repo. I will try my best to review them ASAP.
